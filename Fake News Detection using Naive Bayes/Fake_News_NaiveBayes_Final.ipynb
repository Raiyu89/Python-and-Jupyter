{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fake News Detection with Naive Bayes\n",
        "---\n",
        "This notebook demonstrates how to build a **Naive Bayes classifier** to detect fake vs. real news using a dataset of news articles.\n",
        "We will cover: \n",
        "1. Exploratory Data Analysis (EDA)\n",
        "2. Text Preprocessing & Feature Engineering\n",
        "3. Model Training (Naive Bayes)\n",
        "4. Evaluation (Accuracy, Precision, Recall, F1)\n",
        "5. Visualization (Confusion Matrix, Top Words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn import metrics\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv('Assignment_Data_fake_or_real_news.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df.info())\n",
        "print(df['label'].value_counts())\n",
        "sns.countplot(data=df, x='label')\n",
        "plt.title('Class Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preparation (Title + Text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['content'] = df['title'] + ' ' + df['text']\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['content'], df['label'], \n",
        "                                                    test_size=0.2, random_state=42, stratify=df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Naive Bayes Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words='english', max_df=0.7)),\n",
        "    ('nb', MultinomialNB())\n",
        "])\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print('Accuracy:', accuracy)\n",
        "print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "conf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['FAKE','REAL'], yticklabels=['FAKE','REAL'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature Importance (Top Words per Class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vectorizer = model.named_steps['tfidf']\n",
        "nb = model.named_steps['nb']\n",
        "feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "log_probs = nb.feature_log_prob_\n",
        "\n",
        "top_fake = feature_names[np.argsort(log_probs[0])[-15:]]\n",
        "top_real = feature_names[np.argsort(log_probs[1])[-15:]]\n",
        "\n",
        "print('Top words indicating FAKE news:', top_fake)\n",
        "print('Top words indicating REAL news:', top_real)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Extra EDA: Word Clouds & Text Length Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\\n",
        "\\n",
        "# Word Cloud for FAKE news\\n",
        "fake_text = ' '.join(df[df['label']=='FAKE']['content'])\\n",
        "wc_fake = WordCloud(width=800, height=400, background_color='white').generate(fake_text)\\n",
        "plt.figure(figsize=(10,5))\\n",
        "plt.imshow(wc_fake, interpolation='bilinear')\\n",
        "plt.axis('off')\\n",
        "plt.title('Word Cloud - FAKE News')\\n",
        "plt.show()\\n",
        "\\n",
        "# Word Cloud for REAL news\\n",
        "real_text = ' '.join(df[df['label']=='REAL']['content'])\\n",
        "wc_real = WordCloud(width=800, height=400, background_color='white').generate(real_text)\\n",
        "plt.figure(figsize=(10,5))\\n",
        "plt.imshow(wc_real, interpolation='bilinear')\\n",
        "plt.axis('off')\\n",
        "plt.title('Word Cloud - REAL News')\\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Distribution of article length (in words)\\n",
        "df['text_length'] = df['content'].apply(lambda x: len(x.split()))\\n",
        "sns.histplot(data=df, x='text_length', hue='label', bins=50, kde=True)\\n",
        "plt.title('Distribution of Article Length by Label')\\n",
        "plt.xlabel('Number of Words')\\n",
        "plt.ylabel('Count')\\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ROC Curve & AUC Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\\n",
        "from sklearn.preprocessing import LabelBinarizer\\n",
        "\\n",
        "# Binarize labels (FAKE=0, REAL=1)\\n",
        "lb = LabelBinarizer()\\n",
        "y_test_bin = lb.fit_transform(y_test)\\n",
        "y_pred_proba = model.predict_proba(X_test)[:,1]\\n",
        "\\n",
        "# ROC Curve\\n",
        "fpr, tpr, _ = roc_curve(y_test_bin, y_pred_proba)\\n",
        "roc_auc = auc(fpr, tpr)\\n",
        "\\n",
        "plt.figure(figsize=(7,6))\\n",
        "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\\n",
        "plt.xlim([0.0, 1.0])\\n",
        "plt.ylim([0.0, 1.05])\\n",
        "plt.xlabel('False Positive Rate')\\n",
        "plt.ylabel('True Positive Rate')\\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\\n",
        "plt.legend(loc='lower right')\\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}